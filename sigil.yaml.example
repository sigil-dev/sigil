# Sigil configuration
# https://github.com/sigil-dev/sigil
#
# Copy this file to ~/.config/sigil/sigil.yaml and edit to taste.
# Sigil auto-bootstraps a default config on first run if none exists.
#
# Precedence: CLI flags > environment variables > config file > defaults
# Environment: SIGIL_<SECTION>_<KEY> (e.g. SIGIL_NETWORKING_LISTEN)

# ---------------------------------------------------------------------------
# Authentication
# ---------------------------------------------------------------------------
# When tokens are configured, API endpoints enforce identity and permissions.
# When no tokens are configured, all endpoints are accessible (dev mode).
# See docs/decisions/decision-log.md (D061) for details.
#
# auth:
#   tokens:
#     - token: "secret-token-for-alice"
#       user_id: "alice"
#       name: "Alice"
#       permissions: ["admin:plugins", "workspace:personal"]
#     - token: "secret-token-for-bob"
#       user_id: "bob"
#       name: "Bob"
#       permissions: ["workspace:personal"]

# ---------------------------------------------------------------------------
# Networking
# ---------------------------------------------------------------------------
networking:
  mode: local                       # local | tailscale
  listen: "127.0.0.1:18789"
  enable_hsts: false                # Enable HTTP Strict Transport Security
  rate_limit_rps: 0                 # Requests per second per IP (0 = disabled)
  rate_limit_burst: 0               # Burst size for rate limiting
  # cors_origins:                   # Allowed CORS origins (empty = dev default)
  #   - "http://localhost:5173"
  # dev_csp_connect_src: "http://localhost:18789"  # Tauri dev WebSocket (omit in production)
  # trusted_proxies:                # CIDR ranges of trusted reverse proxies
  #   - "10.0.0.0/8"               # Required when behind a reverse proxy
  #   - "172.16.0.0/12"            # Only X-Forwarded-For from these IPs is trusted
  # tailscale:
  #   hostname: "my-agent"
  #   auth_key: "${TS_AUTHKEY}"
  #   node_auth:
  #     required_tag: "tag:agent-node"

# ---------------------------------------------------------------------------
# Storage
# ---------------------------------------------------------------------------
storage:
  backend: sqlite                   # sqlite (default, only supported backend)

# ---------------------------------------------------------------------------
# LLM Providers
# ---------------------------------------------------------------------------
providers:
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
  # openai:
  #   api_key: "${OPENAI_API_KEY}"
  # google:
  #   api_key: "${GOOGLE_API_KEY}"
  # openrouter:
  #   api_key: "${OPENROUTER_API_KEY}"
  # ollama:
  #   endpoint: "http://localhost:11434"

# ---------------------------------------------------------------------------
# Model configuration
# ---------------------------------------------------------------------------
models:
  default: "anthropic/claude-sonnet-4-5"
  failover:
    - "anthropic/claude-sonnet-4-5"
  budgets:
    per_session_tokens: 100000
    per_hour_usd: 5.00
    per_day_usd: 50.00

# ---------------------------------------------------------------------------
# Sessions
# ---------------------------------------------------------------------------
sessions:
  memory:
    active_window: 20
    compaction:
      strategy: summarize
      summary_model: "anthropic/claude-haiku-4-5"
      batch_size: 50

# ---------------------------------------------------------------------------
# Security
# ---------------------------------------------------------------------------
# Content scanning and detection modes for user input, tool output, and
# assistant output. Each hook can operate in one of three modes:
#   - block   : Reject content flagged by scanner (most restrictive)
#   - flag    : Allow content but mark it for review/logging (default for tool output)
#   - redact  : Allow content but strip detected sensitive data (default for output)
#
# Default mode set in code: input=block, tool=flag, output=redact
#
security:
  scanner:
    # User input scanning mode. Default: block
    # Prevents users from submitting input containing detected sensitive content.
    # Note: Only 'block' is allowed in production. To use 'flag' or 'redact',
    # you MUST explicitly set allow_permissive_input_mode=true AND you accept the
    # security implications. This setting cannot be changed via environment variable.
    input: block

    # Tool output scanning mode. Default: flag
    # Controls how tool results containing injection patterns are handled before
    # being passed to the LLM.
    #
    # Mode behavior:
    # - flag  : Detected injection patterns are logged as warnings but the tool
    #           result is still passed to the LLM unchanged. This is the default
    #           because it prioritizes availability — tools continue to function
    #           even when scanner heuristics fire on benign content. Use this
    #           when tool sources are trusted or false-positive rate is high.
    # - block : Tool results that contain detected injection patterns are rejected
    #           outright; the agent receives an error instead of the tainted
    #           content. Recommended for production deployments where tool sources
    #           are untrusted (e.g., web-fetch, external APIs, user-supplied URLs).
    #           This is the hardened setting that prevents prompt-injection via
    #           malicious tool output from reaching the LLM context.
    # - redact: Detected sensitive patterns are stripped from tool output before
    #           it is passed to the LLM. Preserves partial tool results at the
    #           cost of potentially incomplete context.
    #
    # Production guidance:
    #   If your deployment processes tool results from untrusted sources, set
    #   this to 'block'. The 'flag' default is intentionally availability-first
    #   to avoid breaking workflows during initial rollout; it is NOT suitable
    #   as a long-term security posture when tool sources are not fully trusted.
    tool: flag

    # Assistant output scanning mode. Default: redact
    # Controls how LLM output containing detected sensitive content is handled.
    # - block:  Reject LLM response if it contains detected secrets
    # - flag:   Allow LLM response but mark it for review
    # - redact: Strip detected sensitive patterns from LLM output before returning
    output: redact

    # Allow non-block input scanning modes. Default: false
    # Setting this to true allows input scanning in 'flag' or 'redact' modes,
    # which means user input containing sensitive data would be allowed through
    # with logging or automatic redaction. This is a security policy decision.
    # CRITICAL: This setting is NOT overridable via SIGIL_SECURITY_SCANNER_ALLOW_PERMISSIVE_INPUT_MODE
    # environment variable to prevent env injection from bypassing input controls.
    # allow_permissive_input_mode: false

    # Content size limits — tune for your deployment's model context and security posture.
    # See D064/D074 in docs/decisions/decision-log.md for rationale.
    # All values are in bytes. Min: 65536 (64KB), Max: 10485760 (10MB).
    # These settings cannot be changed via environment variables.
    # limits:
    #   # Maximum content size the scanner accepts (post-normalization).
    #   # Content exceeding this is rejected. Default: 1MB.
    #   # Increase for large-context models (e.g., Gemini 1.5 Pro).
    #   max_content_length: 1048576
    #
    #   # Hard cap applied BEFORE normalization to prevent CPU DoS.
    #   # Must be >= max_content_length. Default: 5MB.
    #   max_pre_norm_content_length: 5242880
    #
    #   # Pre-scanner truncation for tool results. Default: 1MB.
    #   # Should match max_content_length. Truncated before primary scan.
    #   max_tool_result_scan_size: 1048576
    #
    #   # Truncation target for oversized tool results before re-scan. Default: 512KB.
    #   # Must be < max_content_length.
    #   max_tool_content_scan_size: 524288

  # Audit logging controls.
  # audit:
  #   # Fail-closed mode. Default: false (best-effort).
  #   # When true, audit write failures on the ALLOW path cause capability checks
  #   # to return an error (CodeSecurityAuditFailure / 503). Use in compliance-
  #   # critical environments where an unrecorded operation is unacceptable.
  #   # When false (default), audit failures are logged as warnings and operations
  #   # proceed. Consecutive failures escalate the log level from Warn to Error.
  #   fail_closed: false

# ---------------------------------------------------------------------------
# Workspaces
# ---------------------------------------------------------------------------
workspaces:
  personal:
    description: "Default personal workspace"
    members: []
    tools:
      allow: ["*"]
      # deny: ["tool.dangerous_exec"]   # deny patterns evaluated first
    skills: []
  # team:
  #   description: "Shared team workspace"
  #   members: ["alice", "bob"]
  #   bindings:
  #     - channel: "slack"
  #       channel_id: "C12345"
  #   tools:
  #     allow: ["tool.*"]
  #     deny: ["tool.shell_exec"]
  #   skills: ["summarize"]
